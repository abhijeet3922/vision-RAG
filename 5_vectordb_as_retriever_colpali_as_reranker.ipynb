{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhijeet3922/vision-RAG/blob/main/5_vectordb_as_retriever_colpali_as_reranker.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **FAISS vector DB as Retriever (1st pass) and ColPali as Re-ranker (2nd pass)**\n",
        "\n",
        "This notebook goes one step deeper into treating faster vector DB cosine scoring as 1st pass and then employing ColPali as re-ranker to improve accuracy (SOTA late interaction mechanism).\n",
        "\n",
        "- The example in the notebook shows where FAISS vector DB retrieval fails to get correct page on the top, ColPali comes to rescue as a re-ranker to fetch the correct page.\n",
        "\n",
        "- This is certainly very helpful in scaling the solution.\n",
        "\n",
        "\n",
        "Following are the code components for this notebook:\n",
        "1. [Installing Libraries & Imports](#)\n",
        "2. [Loading Visual Language Model (VLM): ColPali](#)\n",
        "3. [Ingest PDF Data as Embeddings](#)\n",
        "4. [Setup FAISS vector DB Index](#)\n",
        "5. [Add Embeddings & Metadata Fields to Index](#)\n",
        "6. [Retrieve best Matching Page from Vector DB](#)\n",
        "7. [Rerank Vector DB output using Colpali as Re-ranker](#)\n",
        "8. [Load Multi-modal LLM: Qwen2.5-VL 3B](#)\n",
        "9. [Prepare Prompt with Retrieved Page](#)\n",
        "10. [Generate Answer: Model Inference](#)\n",
        "\n"
      ],
      "metadata": {
        "id": "5xQBbc17QHai"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Install Libraries & Imports"
      ],
      "metadata": {
        "id": "VxkY_F87DcsR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "skAQJoUE55Kt",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install pdf2image\n",
        "!pip install colpali-engine==0.3.9\n",
        "!sudo apt-get install poppler-utils\n",
        "\n",
        "!pip install faiss-cpu\n",
        "!pip install langchain_community"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from pdf2image import convert_from_path\n",
        "from torch.utils.data import DataLoader\n",
        "from colpali_engine.models import ColPali, ColPaliProcessor"
      ],
      "metadata": {
        "id": "hUzKRXs_Dg-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Loading Visual Language Model (VLM): ColPali"
      ],
      "metadata": {
        "id": "UtGe88-KTTGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"vidore/colpali-v1.3\"\n",
        "\n",
        "model = ColPali.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",  # or \"mps\" if on Apple Silicon\n",
        ").eval()\n",
        "\n",
        "processor = ColPaliProcessor.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "jCsxl9t8GSWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Ingest PDF Data as Embeddings"
      ],
      "metadata": {
        "id": "cFcAdJGLTYLS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "images = convert_from_path('/content/google-alphabet-2024.pdf')\n",
        "print(\"Number of pages:\", len(images))"
      ],
      "metadata": {
        "id": "AOaxEHBaDmHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = DataLoader(images,\n",
        "                        batch_size=1,\n",
        "                        shuffle=False,\n",
        "                        collate_fn=lambda x: processor.process_images(x).to(model.device))"
      ],
      "metadata": {
        "id": "tBq8uccqDtJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = []\n",
        "for batch in tqdm(dataloader):\n",
        "  with torch.no_grad():\n",
        "    batch = {k: v.to(model.device) for k,v in batch.items()}\n",
        "    embeddings = model(**batch)\n",
        "  dataset.extend(list(torch.unbind(embeddings.to(\"cpu\").to(torch.float32))))"
      ],
      "metadata": {
        "id": "_LvJtmY0Dypx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = [\"What is the revenue from Google Cloud for 2023 and 2024 ?\"]\n",
        "\n",
        "batch_queries = processor.process_queries(query).to(model.device)\n",
        "with torch.no_grad():\n",
        "  query_embeddings = model(**batch_queries)"
      ],
      "metadata": {
        "id": "M80wQtdufDbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Setup FAISS vector DB Index"
      ],
      "metadata": {
        "id": "Uh7wyVJh7mr3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
        "from langchain_community.vectorstores import FAISS"
      ],
      "metadata": {
        "id": "EYnBepKYepz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = faiss.IndexFlatL2(128)\n",
        "\n",
        "vector_store = FAISS(\n",
        "    embedding_function=None,\n",
        "    index=index,\n",
        "    docstore=InMemoryDocstore(),\n",
        "    index_to_docstore_id={},\n",
        ")"
      ],
      "metadata": {
        "id": "fMXmyBkRevZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Add Embeddings & Metadata Fields to Index"
      ],
      "metadata": {
        "id": "F-xDHpSL8Cm5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import PyPDF2\n",
        "# from PIL import Image\n",
        "\n",
        "file_name = '/content/google-alphabet-2024.pdf'\n",
        "\n",
        "all_images = []\n",
        "for i, img in enumerate(images):\n",
        "  page_name = \"/content/\"+str(i)+\".pdf\"\n",
        "  all_images.append(page_name)\n",
        "\n",
        "all_embeddings = [l for l in dataset]\n",
        "list_of_tuple = []\n",
        "for img, embd in zip(all_images, all_embeddings):\n",
        "  for i in range(0,1030):\n",
        "    list_of_tuple.append((img, embd[i]))\n",
        "\n",
        "metadata = []\n",
        "uids = []\n",
        "count=0\n",
        "for i in range(0,len(images)):\n",
        "  for j in range(0,1030):\n",
        "    file_name = file_name\n",
        "    page_name = file_name.split(\"/\")[-1].split(\".\")[0]+\"_\"+str(i)+\".pdf\"\n",
        "    patch_num = j\n",
        "    uid = count\n",
        "    metadata.append({\"file_name\":file_name, \"page_name\": page_name, \"patch_num\":patch_num,\"uid\":uid})\n",
        "    uids.append(uid)\n",
        "    count += 1"
      ],
      "metadata": {
        "id": "pY9Ou5bTgEFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids = vector_store.add_embeddings(text_embeddings= list_of_tuple, metadatas=metadata, ids = uids)"
      ],
      "metadata": {
        "id": "23GugBJSgqBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Retrieve best Matching Page from Vector DB"
      ],
      "metadata": {
        "id": "N6aASWUE8QI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "page_name_list = []\n",
        "for i in range(0,query_embeddings[0].shape[0]):\n",
        "    vec = query_embeddings[0][i].tolist()\n",
        "    results = vector_store.similarity_search_by_vector(vec, k=3)\n",
        "    for doc in results:\n",
        "       page_name_list.append(doc.metadata['page_name'])"
      ],
      "metadata": {
        "id": "N9CEgntHgtNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_list_items = set(page_name_list)\n",
        "page_dict={}\n",
        "\n",
        "for list_item in unique_list_items:\n",
        "    page_dict[list_item]=page_name_list.count(list_item)\n",
        "sorted_page_dict = dict(sorted(page_dict.items(),key=lambda item: item[1], reverse=True))\n",
        "top_pages = list(sorted_page_dict.keys())\n",
        "top_pages"
      ],
      "metadata": {
        "id": "ZT9QvrnvhkJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Rerank Vector DB output using Colpali as Re-ranker\n",
        "*  Get page embeddings from DB\n",
        "*  Call score function using colpali embeddings"
      ],
      "metadata": {
        "id": "-_cXSeBTx703"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_doc_embeddings(page_names_list):\n",
        "  doc_vectors = []\n",
        "  for page_name in page_names_list:\n",
        "\n",
        "      page_num = page_name.split(\"_\")[-1].split(\".\")[0]\n",
        "      page_num = int(page_num)\n",
        "      doc_vectors.append(vector_store.index.reconstruct_n(page_num*1030,1030))\n",
        "\n",
        "  doc_embd = torch.from_numpy(np.stack(doc_vectors))\n",
        "  return doc_embd"
      ],
      "metadata": {
        "id": "-EkNTSaKv_ls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_embd = get_doc_embeddings(top_pages)\n",
        "print(doc_embd.shape)"
      ],
      "metadata": {
        "id": "0TDshCjuweWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def score(query_embedding, dataset):\n",
        "  scores = processor.score_multi_vector(query_embedding, dataset)\n",
        "  scores = np.array(scores)\n",
        "  matched_pages = scores.flatten().argsort()[::-1]\n",
        "  return scores, matched_pages\n",
        "\n",
        "scores, matched_pages = score(list(torch.unbind(query_embeddings.to(\"cpu\").to(torch.float32))),doc_embd)\n",
        "print('Similarity scores: ', scores[0], '\\n', 'Matched pages: ', [top_pages[matched_pages[i]] for i in matched_pages])"
      ],
      "metadata": {
        "id": "aIufjEnGwuA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_pages[matched_pages[0]]"
      ],
      "metadata": {
        "id": "-1Ufjmfv21cg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize=(16, 12))\n",
        "ax.imshow(images[int(top_pages[matched_pages[0]].split(\"_\")[-1].split(\".\")[0])])\n",
        "ax.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6VKm2anA1-FZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Load Multi-modal LLM: Qwen2.5-VL 3B"
      ],
      "metadata": {
        "id": "2bRtac38T7VS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install qwen-vl-utils==0.0.08"
      ],
      "metadata": {
        "id": "P1v7ix940s7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from qwen_vl_utils import process_vision_info\n",
        "from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor"
      ],
      "metadata": {
        "id": "hPAQ_Sqt03XN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# default: Load the model on the available device(s)\n",
        "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
        "    \"Qwen/Qwen2.5-VL-3B-Instruct\", torch_dtype=\"auto\", device_map=\"auto\"\n",
        ")\n",
        "\n",
        "\n",
        "# default processer\n",
        "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-3B-Instruct\")"
      ],
      "metadata": {
        "id": "dwMQEZ_0EQdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Prepare Prompt with Retrieved Page"
      ],
      "metadata": {
        "id": "d9960zcpUJ6S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "\n",
        "    {\"role\": \"user\",\n",
        "     \"content\": [\n",
        "         {\"type\": \"image\",\n",
        "          \"image\": images[int(top_pages[matched_pages[0]].split(\"_\")[-1].split(\".\")[0])],\n",
        "          \"resized_height\": 1024,\n",
        "          \"resized_width\": 1024,\n",
        "         },\n",
        "        {\"type\": \"text\", \"text\": \"What is the revenue from Google Cloud for 2023 and 2024 ?\"}]},\n",
        "\n",
        "]"
      ],
      "metadata": {
        "id": "2tSh4lxeEc3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparation for inference\n",
        "text = processor.apply_chat_template(\n",
        "    messages, tokenize=False, add_generation_prompt=True\n",
        ")\n",
        "image_inputs, video_inputs = process_vision_info(messages)\n",
        "inputs = processor(\n",
        "    text=[text],\n",
        "    images=image_inputs,\n",
        "    videos=video_inputs,\n",
        "    padding=True,\n",
        "    return_tensors=\"pt\",\n",
        ")\n",
        "inputs = inputs.to(\"cuda\")"
      ],
      "metadata": {
        "id": "p4xfx8faEnyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10. Generate Answer: Model Inference"
      ],
      "metadata": {
        "id": "SN432852UWWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference: Generation of the output\n",
        "\n",
        "with torch.no_grad():\n",
        "  generated_ids = model.generate(**inputs, max_new_tokens=64)\n",
        "\n",
        "generated_ids_trimmed = [\n",
        "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
        "]\n",
        "output_text = processor.batch_decode(\n",
        "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
        ")\n",
        "print(output_text[0])"
      ],
      "metadata": {
        "id": "1KoW-ezJEoZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Key Takeways**\n",
        "\n",
        "* Using ColPali (VLM) as re-ranker is the best practical way to achieve best performance while scaling up your system."
      ],
      "metadata": {
        "id": "FsoT_fQANYRk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Current State of Vector DBs**\n",
        "\n",
        "1.   Qdrant vector DB has an offering with ColPali, ColQwen integration (Jan, 2025)\n",
        "2.   Weaviate DB started offering multi-vector embedding support from v1.29 (Feb, 2025)\n",
        "3. Elasticsearch has rolled out ColPali implementation in March, 2025.\n",
        "4. Milvus vector DB completed integration with ColPali (multi-vector) by May, 2025.\n"
      ],
      "metadata": {
        "id": "6V9Kpo0HNqck"
      }
    }
  ]
}